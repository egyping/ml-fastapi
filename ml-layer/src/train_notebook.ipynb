{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c399b6f7",
   "metadata": {},
   "source": [
    "\n",
    "# Train Notebook — EG Property Estimator\n",
    "\n",
    "This notebook walks through the **machine learning flow** used in this project:\n",
    "1. Load and inspect the dataset (CSV).\n",
    "2. Clean & prepare the data.\n",
    "3. Feature engineering (turn raw columns into stronger signals).\n",
    "4. Build a modeling pipeline (preprocessing + model).\n",
    "5. Cross-validation and a **baseline blend** for stability.\n",
    "6. City-level calibration (bias correction).\n",
    "7. Final evaluation + simple visualizations.\n",
    "8. Save the trained model (`.joblib`) + artifacts for serving with **FastAPI**.\n",
    "\n",
    "> **Inputs:** a cleaned CSV of listing data (`Eg_RealState_Data_Cleaned.csv`) placed at `ml-layer/data/`  \n",
    "> **Outputs:** a trained model (`ppm2_model.joblib`), baseline median tables, and a city-bias file, all saved to `ml-layer/artifacts/` and consumed by the API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c767f",
   "metadata": {},
   "source": [
    "\n",
    "## How to run this notebook\n",
    "\n",
    "- If you don't have Jupyter:\n",
    "  ```bash\n",
    "  # (activate your venv first)\n",
    "  pip install notebook  # or: pip install jupyterlab\n",
    "  jupyter notebook       # or: jupyter lab\n",
    "  ```\n",
    "- Open **`train_notebook.ipynb`** and run cells top-to-bottom.\n",
    "- Make sure your dataset is at: `ml-layer/data/Eg_RealState_Data_Cleaned.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9223fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Optional: MLflow tracking (uncomment if you'd like to log runs)\n",
    "# import mlflow\n",
    "# import mlflow.sklearn\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd()  # adjust if running from a different folder\n",
    "DATA_PATH = PROJECT_ROOT / \"ml-layer\" / \"data\" / \"Eg_RealState_Data_Cleaned.csv\"\n",
    "ARTIFACT_DIR = PROJECT_ROOT / \"ml-layer\" / \"artifacts\"\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Data path:', DATA_PATH)\n",
    "print('Artifacts will be saved to:', ARTIFACT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0832be",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load and quickly inspect the data\n",
    "\n",
    "We expect at least these columns:\n",
    "- `price`, `area`, `bedrooms`, `bathrooms`, `type`, `city`, `region`\n",
    "- (optional) `level`, `furnished`, `rent`\n",
    "\n",
    "We'll load the CSV and do a tiny sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fef560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Raw shape:', df.shape)\n",
    "print('Columns:', list(df.columns))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccc1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic info & nulls\n",
    "display(df.describe(include='all').T.head(20))\n",
    "df.isna().mean().sort_values(ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece73810",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Cleaning & Feature Engineering (high level)\n",
    "\n",
    "**Cleaning**\n",
    "- Keep **sale** listings only (if `rent` exists, filter where `rent == \"no\"`).\n",
    "- Drop obviously bad city tokens (`\"\"`, `\"nan\"`, `\"none\"`, `\"null\"`, `\"v\"`).\n",
    "- Bucket **rare** regions into a single `\"__other__\"` bucket.\n",
    "- Remove **outliers** by trimming **price-per-m² (ppm²)**:\n",
    "  - Global trim (1–99th percentiles)\n",
    "  - Per-city robust trim (5–95th) if the city has enough data (≥50 rows)\n",
    "\n",
    "**Feature Engineering**\n",
    "- Cast numeric-like columns to numbers; coerce unknowns to NaN.\n",
    "- `level_num` (floor parsed from string), `is_ground` (1 if ground).\n",
    "- `furnished_bin` (1 for \"yes\", 0 for \"no\").\n",
    "- Ratios & transforms: `area_per_bedroom`, `bathrooms_per_bedroom`, `rooms_total`, `bed_bath_ratio`, `log_area`, `sqrt_area`, `area_per_room`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cacaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_and_engineer(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # ensure presence of key cols\n",
    "    for c in [\"type\",\"city\",\"region\",\"level\",\"furnished\",\"bedrooms\",\"bathrooms\",\"area\",\"price\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    # standardize strings\n",
    "    for c in [\"type\",\"city\",\"region\",\"level\",\"furnished\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # coerce numbers\n",
    "    for c in [\"area\",\"bedrooms\",\"bathrooms\",\"price\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # level -> numeric + ground flag\n",
    "    def level_to_num(s):\n",
    "        if isinstance(s, str):\n",
    "            if \"ground\" in s:\n",
    "                return 0.0\n",
    "            try:\n",
    "                return float(s)\n",
    "            except:\n",
    "                return np.nan\n",
    "        try:\n",
    "            return float(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    df[\"level_num\"] = df[\"level\"].apply(level_to_num)\n",
    "    df[\"is_ground\"] = (df[\"level\"].fillna(\"\").str.contains(\"ground\")).astype(float)\n",
    "\n",
    "    # furnished -> binary\n",
    "    df[\"furnished_bin\"] = df[\"furnished\"].map({\"yes\":1.0, \"no\":0.0}).fillna(0.0).astype(float)\n",
    "\n",
    "    # engineered numeric\n",
    "    safe_bed = df[\"bedrooms\"].replace(0, np.nan)\n",
    "    safe_bath = df[\"bathrooms\"].replace(0, np.nan)\n",
    "    safe_area = df[\"area\"].clip(lower=1)\n",
    "\n",
    "    df[\"area_per_bedroom\"]     = df[\"area\"] / safe_bed\n",
    "    df[\"bathrooms_per_bedroom\"] = df[\"bathrooms\"] / safe_bed\n",
    "    df[\"rooms_total\"]          = df[\"bedrooms\"] + df[\"bathrooms\"]\n",
    "    df[\"log_area\"]             = np.log1p(safe_area)\n",
    "    df[\"sqrt_area\"]            = np.sqrt(safe_area)\n",
    "    df[\"bed_bath_ratio\"]       = df[\"bedrooms\"] / safe_bath\n",
    "    df[\"area_per_room\"]        = df[\"area\"] / df[\"rooms_total\"]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2a) Keep sales only if 'rent' exists\n",
    "if \"rent\" in df.columns:\n",
    "    df[\"rent\"] = df[\"rent\"].astype(str).str.lower()\n",
    "    df = df[df[\"rent\"] == \"no\"].copy()\n",
    "\n",
    "# 2b) Drop rows missing key fields\n",
    "required = [\"price\",\"area\",\"bedrooms\",\"bathrooms\",\"type\",\"city\",\"region\"]\n",
    "df = df.dropna(subset=[c for c in required if c in df.columns]).copy()\n",
    "\n",
    "# 2c) Feature engineering\n",
    "df = normalize_and_engineer(df)\n",
    "\n",
    "# 2d) Global ppm² trimming\n",
    "df[\"ppm2_tmp\"] = df[\"price\"] / df[\"area\"].clip(lower=1)\n",
    "lo, hi = df[\"ppm2_tmp\"].quantile([0.01, 0.99])\n",
    "df = df[(df[\"ppm2_tmp\"] >= lo) & (df[\"ppm2_tmp\"] <= hi)].copy()\n",
    "\n",
    "# 2e) Remove bad city tokens\n",
    "bad_cities = {\"v\",\"\", \"nan\",\"none\",\"null\"}\n",
    "df = df[~df[\"city\"].isin(bad_cities)].copy()\n",
    "\n",
    "# 2f) Bucket rare regions\n",
    "vc = df[\"region\"].value_counts()\n",
    "keep_regions = set(vc[vc >= 30].index)\n",
    "df[\"region\"] = df[\"region\"].where(df[\"region\"].isin(keep_regions), \"__other__\")\n",
    "\n",
    "# 2g) Per-city robust trimming (5-95) for cities with adequate data (>=50)\n",
    "city_counts = df.groupby(\"city\")[\"ppm2_tmp\"].transform(\"count\")\n",
    "city_lo = df.groupby(\"city\")[\"ppm2_tmp\"].transform(lambda s: s.quantile(0.05))\n",
    "city_hi = df.groupby(\"city\")[\"ppm2_tmp\"].transform(lambda s: s.quantile(0.95))\n",
    "use_city = city_counts >= 50\n",
    "df = df[(~use_city & (df[\"ppm2_tmp\"] >= lo) & (df[\"ppm2_tmp\"] <= hi)) |\n",
    "        (use_city  & (df[\"ppm2_tmp\"] >= city_lo) & (df[\"ppm2_tmp\"] <= city_hi))].copy()\n",
    "\n",
    "df = df.drop(columns=[\"ppm2_tmp\"])\n",
    "print('Post-clean shape:', df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589e0af",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Category encoding\n",
    "\n",
    "- `type` is **low-cardinality** → One-Hot Encoding  \n",
    "- `city` and `region` are **high-cardinality** → **Smoothed Target Encoding (TE)**\n",
    "\n",
    "The TE encodes each category to a smoothed mean of the target (ppm²), learned **inside the training folds** to avoid leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daafe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SmoothedTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, alpha=10.0):\n",
    "        self.cols = cols\n",
    "        self.alpha = alpha\n",
    "        self.prior_ = None\n",
    "        self.maps_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if y is None:\n",
    "            raise ValueError(\"SmoothedTargetEncoder requires y during fit.\")\n",
    "        X = X.copy()\n",
    "        self.prior_ = float(np.mean(y))\n",
    "        self.maps_ = {}\n",
    "        for c in self.cols:\n",
    "            s = pd.Series(X[c].astype(str).values, name=c)\n",
    "            dfc = pd.DataFrame({c: s, \"y\": y})\n",
    "            grp = dfc.groupby(c)[\"y\"].agg([\"sum\",\"count\"])\n",
    "            enc = (grp[\"sum\"] + self.prior_ * self.alpha) / (grp[\"count\"] + self.alpha)\n",
    "            self.maps_[c] = enc.to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        out = []\n",
    "        for c in self.cols:\n",
    "            m = self.maps_.get(c, {})\n",
    "            out.append(X[c].astype(str).map(m).fillna(self.prior_).to_numpy().reshape(-1,1))\n",
    "        return np.concatenate(out, axis=1)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array([f\"te_{c}\" for c in self.cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d3c37",
   "metadata": {},
   "source": [
    "\n",
    "## 4. A robust baseline (medians)\n",
    "\n",
    "We compute **median ppm²** at progressively broader levels:\n",
    "1. `(city, region, type)`\n",
    "2. `(city, region)`\n",
    "3. `(city)`\n",
    "4. Global median\n",
    "\n",
    "At prediction time we try level 1; if missing, fall back to 2 → 3 → global.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f07185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_ppm2_baseline(X: pd.DataFrame, yppm2: pd.Series):\n",
    "    dfb = pd.DataFrame({\n",
    "        \"city\": X[\"city\"].astype(str),\n",
    "        \"region\": X[\"region\"].astype(str),\n",
    "        \"type\": X[\"type\"].astype(str),\n",
    "        \"ppm2\": yppm2.astype(float)\n",
    "    })\n",
    "    m_crt = dfb.groupby([\"city\",\"region\",\"type\"])[\"ppm2\"].median().rename(\"m_crt\").reset_index()\n",
    "    m_cr  = dfb.groupby([\"city\",\"region\"])[\"ppm2\"].median().rename(\"m_cr\").reset_index()\n",
    "    m_c   = dfb.groupby([\"city\"])[\"ppm2\"].median().rename(\"m_c\").reset_index()\n",
    "    m_g   = float(dfb[\"ppm2\"].median())\n",
    "    return {\"m_crt\": m_crt, \"m_cr\": m_cr, \"m_c\": m_c, \"m_g\": m_g}\n",
    "\n",
    "def predict_ppm2_baseline(X: pd.DataFrame, stats: dict) -> np.ndarray:\n",
    "    s = X[[\"city\",\"region\",\"type\"]].astype(str).copy()\n",
    "    out = s.merge(stats[\"m_crt\"], on=[\"city\",\"region\",\"type\"], how=\"left\")\n",
    "    out = out.merge(stats[\"m_cr\"], on=[\"city\",\"region\"], how=\"left\")\n",
    "    out[\"ppm2\"] = out[\"m_crt\"].fillna(out[\"m_cr\"])\n",
    "    out = out.merge(stats[\"m_c\"], on=[\"city\"], how=\"left\")\n",
    "    out[\"ppm2\"] = out[\"ppm2\"].fillna(out[\"m_c\"]).fillna(stats[\"m_g\"])\n",
    "    return out[\"ppm2\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a273d",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train/Validation split and pipeline\n",
    "\n",
    "- **Target**: `ppm² = price / area`\n",
    "- **Pipeline**:\n",
    "  - Numeric features: passthrough\n",
    "  - `type`: One-Hot Encoding\n",
    "  - `city`, `region`: Smoothed Target Encoding\n",
    "  - Regressor: HistGradientBoostingRegressor (with `loss=\"absolute_error\"`)\n",
    "  - Wrap in `TransformedTargetRegressor` to learn on **log1p(ppm²)** for stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592cb148",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select columns\n",
    "num_feats_num = [\n",
    "    \"area\",\"bedrooms\",\"bathrooms\",\n",
    "    \"level_num\",\"is_ground\",\"furnished_bin\",\n",
    "    \"area_per_bedroom\",\"bathrooms_per_bedroom\",\"rooms_total\",\n",
    "    \"log_area\",\"sqrt_area\",\"bed_bath_ratio\",\"area_per_room\",\n",
    "]\n",
    "cat_ohe_cols = [\"type\"]\n",
    "cat_te_cols  = [\"city\",\"region\"]\n",
    "\n",
    "cols_needed = [\"price\"] + num_feats_num + cat_ohe_cols + cat_te_cols\n",
    "dfm = df[cols_needed].dropna().copy()\n",
    "\n",
    "# Base arrays\n",
    "X = dfm.drop(columns=[\"price\"])\n",
    "y_price = dfm[\"price\"].astype(float)\n",
    "area_clip = X[\"area\"].clip(lower=1).astype(float)\n",
    "y_ppm2 = (y_price / area_clip).astype(float)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, yppm2_train, yppm2_test, yprice_train, yprice_test, area_train, area_test = train_test_split(\n",
    "    X, y_ppm2, y_price, area_clip, test_size=0.15, random_state=42, stratify=X[\"city\"]\n",
    ")\n",
    "\n",
    "# Preprocessor + model\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", num_feats_num),\n",
    "        (\"type_ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_ohe_cols),\n",
    "        (\"cat_te\", SmoothedTargetEncoder(cols=cat_te_cols, alpha=10.0), cat_te_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reg = HistGradientBoostingRegressor(\n",
    "    loss=\"absolute_error\",\n",
    "    max_depth=None,\n",
    "    learning_rate=0.06,\n",
    "    max_iter=900,\n",
    "    l2_regularization=0.0,\n",
    "    early_stopping=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"reg\", TransformedTargetRegressor(regressor=reg, func=np.log1p, inverse_func=np.expm1)),\n",
    "])\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd9617",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Cross-validation & Blending\n",
    "\n",
    "We blend the model ppm² with the median-baseline ppm²:\n",
    "\n",
    "\\[ ppm²\\_blend = \\alpha \\cdot ppm²\\_{model} + (1-\\alpha) \\cdot ppm²\\_{baseline} \\]\n",
    "\n",
    "We pick **α** by 5-fold cross-validation on the training split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALPHA_GRID = [0.5, 0.6, 0.7, 0.8]\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "alpha_rmse = {a: [] for a in ALPHA_GRID}\n",
    "\n",
    "for tr_idx, va_idx in kf.split(X_train):\n",
    "    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
    "    yppm2_tr, yppm2_va = yppm2_train.iloc[tr_idx], yppm2_train.iloc[va_idx]\n",
    "    yprice_va = yprice_train.iloc[va_idx]\n",
    "    area_va = area_train.iloc[va_idx]\n",
    "\n",
    "    model.fit(X_tr, yppm2_tr)\n",
    "    pred_ppm2_va_model = model.predict(X_va)\n",
    "\n",
    "    stats = fit_ppm2_baseline(X_tr, yppm2_tr)\n",
    "    pred_ppm2_va_base = predict_ppm2_baseline(X_va, stats)\n",
    "\n",
    "    for a in ALPHA_GRID:\n",
    "        pred_price_va = (a * pred_ppm2_va_model + (1 - a) * pred_ppm2_va_base) * area_va.values\n",
    "        rmse = root_mean_squared_error(yprice_va, pred_price_va)\n",
    "        alpha_rmse[a].append(rmse)\n",
    "\n",
    "best_alpha = min(ALPHA_GRID, key=lambda a: np.mean(alpha_rmse[a]))\n",
    "cv_rmse_mean = float(np.mean(alpha_rmse[best_alpha]))\n",
    "cv_rmse_std  = float(np.std(alpha_rmse[best_alpha]))\n",
    "\n",
    "print(\"Best alpha:\", best_alpha)\n",
    "print(f\"CV RMSE mean={cv_rmse_mean:,.0f} (±{cv_rmse_std:,.0f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb38e7c",
   "metadata": {},
   "source": [
    "\n",
    "## 7. City-level bias calibration (OOF)\n",
    "\n",
    "We compute a multiplicative correction per city using **out-of-fold** predictions:\n",
    "\\[ bias(city) = \\mathrm{median}\\_{OOF} \\left( \\frac{y\\_{price}}{\\hat{y}\\_{price}} \\right) \\]\n",
    "We clip it to [0.6, 1.6] for stability and apply it after blending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "city_ratios = {}\n",
    "for tr_idx, va_idx in kf.split(X_train):\n",
    "    X_tr, X_va = X_train.iloc[tr_idx], X_train.iloc[va_idx]\n",
    "    yppm2_tr, yppm2_va = yppm2_train.iloc[tr_idx], yppm2_train.iloc[va_idx]\n",
    "    yprice_va = yprice_train.iloc[va_idx]\n",
    "    area_va = area_train.iloc[va_idx]\n",
    "\n",
    "    model.fit(X_tr, yppm2_tr)\n",
    "    pred_ppm2_va_model = model.predict(X_va)\n",
    "    stats = fit_ppm2_baseline(X_tr, yppm2_tr)\n",
    "    pred_ppm2_va_base = predict_ppm2_baseline(X_va, stats)\n",
    "\n",
    "    pred_price_va = (best_alpha * pred_ppm2_va_model + (1 - best_alpha) * pred_ppm2_va_base) * area_va.values\n",
    "    ratio = (yprice_va / np.maximum(pred_price_va, 1.0)).to_numpy()\n",
    "\n",
    "    for city, r in zip(X_va[\"city\"].astype(str).to_numpy(), ratio):\n",
    "        city_ratios.setdefault(city, []).append(r)\n",
    "\n",
    "city_bias = {c: float(np.clip(np.median(rs), 0.6, 1.6)) for c, rs in city_ratios.items()}\n",
    "len(city_bias), list(city_bias.items())[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021413c",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Final fit & evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3edf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit on full training set\n",
    "model.fit(X_train, yppm2_train)\n",
    "\n",
    "# Test predictions\n",
    "pred_ppm2_test_model = model.predict(X_test)\n",
    "stats_full = fit_ppm2_baseline(X_train, yppm2_train)\n",
    "pred_ppm2_test_base = predict_ppm2_baseline(X_test, stats_full)\n",
    "\n",
    "pred_price_test = (best_alpha * pred_ppm2_test_model + (1 - best_alpha) * pred_ppm2_test_base) * area_test.values\n",
    "bias_vec = np.array([city_bias.get(c, 1.0) for c in X_test[\"city\"].astype(str).to_numpy()])\n",
    "pred_price_test_adj = pred_price_test * bias_vec\n",
    "\n",
    "def eval_metrics(y_true_price, pred_price):\n",
    "    rmse = root_mean_squared_error(y_true_price, pred_price)\n",
    "    mae  = mean_absolute_error(y_true_price, pred_price)\n",
    "    mape = (np.abs((y_true_price - pred_price) / np.maximum(y_true_price, 1.0))).mean() * 100\n",
    "    return rmse, mae, mape\n",
    "\n",
    "rmse_raw, mae_raw, mape_raw = eval_metrics(yprice_test, pred_price_test)\n",
    "rmse, mae, mape = eval_metrics(yprice_test, pred_price_test_adj)\n",
    "\n",
    "print(f\"Pre-bias Test RMSE={rmse_raw:,.0f} | MAE={mae_raw:,.0f} | MAPE={mape_raw:.2f}%\")\n",
    "print(f\"Post-bias Test RMSE={rmse:,.0f} | MAE={mae:,.0f} | MAPE={mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f850c",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Visualizations\n",
    "\n",
    "We’ll plot:\n",
    "- **Predicted vs Actual** (price EGP)\n",
    "- **Residuals histogram**\n",
    "- **Top-10 cities by MAPE** (bar chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicted vs Actual\n",
    "plt.figure()\n",
    "plt.scatter(yprice_test, pred_price_test_adj, alpha=0.4)\n",
    "mx = float(max(yprice_test.max(), pred_price_test_adj.max()))\n",
    "plt.plot([0, mx], [0, mx])\n",
    "plt.xlabel(\"Actual Price (EGP)\")\n",
    "plt.ylabel(\"Predicted Price (EGP)\")\n",
    "plt.title(\"Predicted vs Actual (Test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8baa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Residuals histogram\n",
    "resid = yprice_test - pred_price_test_adj\n",
    "plt.figure()\n",
    "plt.hist(resid, bins=50)\n",
    "plt.xlabel(\"Residual (Actual - Predicted) [EGP]\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residuals Histogram (Test)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed3a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Worst cities by MAPE (top 10)\n",
    "dfm_plot = pd.DataFrame({\n",
    "    \"city\": X_test[\"city\"].values,\n",
    "    \"y\": yprice_test.values,\n",
    "    \"p\": pred_price_test_adj\n",
    "})\n",
    "\n",
    "def agg(g):\n",
    "    e = g[\"y\"] - g[\"p\"]\n",
    "    mae = np.mean(np.abs(e))\n",
    "    rmse = np.sqrt(np.mean(e ** 2))\n",
    "    mape = np.mean(np.abs(e) / np.maximum(g[\"y\"], 1.0)) * 100\n",
    "    return pd.Series({\"n\": len(g), \"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape})\n",
    "\n",
    "out = dfm_plot.groupby(\"city\").apply(agg).sort_values(\"MAPE\", ascending=False).head(10)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(out.index.astype(str), out[\"MAPE\"].values)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"MAPE (%)\")\n",
    "plt.title(\"Top-10 Cities by MAPE (Test)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "out.round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6f475",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Save artifacts for serving\n",
    "\n",
    "We save:\n",
    "- **Model** (`ppm2_model.joblib`): the sklearn Pipeline that predicts **ppm²**\n",
    "- **Baseline medians**: `m_crt.csv`, `m_cr.csv`, `m_c.csv`, and `m_g.txt`\n",
    "- **City bias**: `city_bias.csv`\n",
    "\n",
    "> The FastAPI app loads these once at startup and exposes **`POST /predict`** for the frontend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c53094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model\n",
    "model_path = ARTIFACT_DIR / \"ppm2_model.joblib\"\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Save baseline stats\n",
    "stats_dir = ARTIFACT_DIR / \"baseline_stats\"\n",
    "stats_dir.mkdir(exist_ok=True, parents=True)\n",
    "stats_full[\"m_crt\"].to_csv(stats_dir / \"m_crt.csv\", index=False)\n",
    "stats_full[\"m_cr\"].to_csv(stats_dir / \"m_cr.csv\", index=False)\n",
    "stats_full[\"m_c\"].to_csv(stats_dir / \"m_c.csv\", index=False)\n",
    "(stats_dir / \"m_g.txt\").write_text(str(stats_full[\"m_g\"]))\n",
    "\n",
    "# Save city bias\n",
    "pd.Series(city_bias, name=\"bias\").to_csv(ARTIFACT_DIR / \"city_bias.csv\")\n",
    "\n",
    "print(\"Saved:\", model_path)\n",
    "print(\"Saved baseline stats to:\", stats_dir)\n",
    "print(\"Saved city_bias.csv to:\", ARTIFACT_DIR / \"city_bias.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a5357",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix — Where this logic lives in the repo\n",
    "\n",
    "- `ml-layer/src/utils.py` — contains `normalize_and_engineer` for feature engineering (mirrored here).\n",
    "- `ml-layer/src/encoders.py` — contains `SmoothedTargetEncoder` (mirrored here).\n",
    "- `ml-layer/src/predict.py` — loads `ppm2_model.joblib`, median stats, and city bias; exposes a class used by FastAPI (`PriceEstimator`).\n",
    "\n",
    "In production, the **FastAPI** app imports `PriceEstimator` and exposes **`/predict`**, and the **React (Vite)** frontend posts form data to it and displays the result.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
